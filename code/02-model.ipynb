{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e635b4-7f51-4771-b503-3991eb4065e4",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "## Part 2: Modeling\n",
    "\n",
    "Model data for fun and profit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99841833-554e-40b2-b472-4a57b0cf38be",
   "metadata": {},
   "source": [
    "### 0. Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50529b0d-9aea-49b6-9429-7f6b910de60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# pipelines, gridsearch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# custom\n",
    "import ipynb_utils as ipyutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b96d9fe-61c8-4b4e-8909-50c00610ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_json('../data/scrapes-clean.json', orient='index')\n",
    "\n",
    "# convert time to datetime object\n",
    "df['time'] = pd.to_datetime(df['time'], format=ipyutils.DATE_FMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6020c18d-2747-4806-bdfd-94cdda105993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>body-text</th>\n",
       "      <th>title-cc</th>\n",
       "      <th>title-wc</th>\n",
       "      <th>body-cc</th>\n",
       "      <th>body-wc</th>\n",
       "      <th>media</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>Saturn Return MEGATHREAD - we've been getting ...</td>\n",
       "      <td></td>\n",
       "      <td>214</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>MERCURY RX INFOGRAPHIC: Taurus/Gemini, Apr-Jun...</td>\n",
       "      <td></td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>CHANI app issues?</td>\n",
       "      <td>I just downloaded the CHANI app to try out and...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>221</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>Is Mercury in Aquarius in the 6th House as pow...</td>\n",
       "      <td>Not new to the deeper parts of astrology but t...</td>\n",
       "      <td>86</td>\n",
       "      <td>17</td>\n",
       "      <td>314</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>What is the proper orb for a sextile?</td>\n",
       "      <td>What is the proper and respective orb for a se...</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>224</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time                                              title  \\\n",
       "0 2022-02-01  Saturn Return MEGATHREAD - we've been getting ...   \n",
       "1 2022-06-01  MERCURY RX INFOGRAPHIC: Taurus/Gemini, Apr-Jun...   \n",
       "2 2022-08-30                                  CHANI app issues?   \n",
       "4 2022-08-30  Is Mercury in Aquarius in the 6th House as pow...   \n",
       "5 2022-08-30              What is the proper orb for a sextile?   \n",
       "\n",
       "                                           body-text  title-cc  title-wc  \\\n",
       "0                                                          214        36   \n",
       "1                                                           51         8   \n",
       "2  I just downloaded the CHANI app to try out and...        17         3   \n",
       "4  Not new to the deeper parts of astrology but t...        86        17   \n",
       "5  What is the proper and respective orb for a se...        37         8   \n",
       "\n",
       "   body-cc  body-wc  media  comments  \n",
       "0        0        0      0       330  \n",
       "1        0        0      0        22  \n",
       "2      221       50      0         4  \n",
       "4      314       59      0         8  \n",
       "5      224       45      0         8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that all looks good...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d68d3abe-7eda-4afc-aad8-c0fb1bdc3be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time         datetime64[ns]\n",
       "title                object\n",
       "body-text            object\n",
       "title-cc              int64\n",
       "title-wc              int64\n",
       "body-cc               int64\n",
       "body-wc               int64\n",
       "media                 int64\n",
       "comments              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... and that the right datatypes are showing\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ea98a-32ed-4d4f-9d19-1524ee3836ae",
   "metadata": {},
   "source": [
    "### 0.5. Problem Statement\n",
    "\n",
    "What characteristics of a post on Reddit are most predictive of the overall interaction on a thread (as measured by number of comments)?\n",
    "\n",
    "Model will predict whether or not a given Reddit post will have above or below the median number of comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa388c-79f9-48bd-bb58-3b48e0aa7a62",
   "metadata": {},
   "source": [
    "### 1. Generate Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50e06b6-908f-48bf-9da0-45e59538381f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# median comments\n",
    "median = np.median(df['comments'])\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb5882c-281f-4590-87a3-9d02cd06c31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    993\n",
       "1    979\n",
       "Name: comments_gt_median, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target column\n",
    "df['comments_gt_median'] = (df['comments'] > median).astype(int)\n",
    "df['comments_gt_median'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4b475-21fc-492b-8192-0e8fb9e2e5e9",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "Baseline is just about **50%** (we are using median)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421586c-0e19-4b86-8f58-b8ebdd3d751c",
   "metadata": {},
   "source": [
    "### 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ff9ecee-a22a-42de-bba7-c90e944b8bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1479, 8), (493, 8), (1479,), (493,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_target = 'comments_gt_median'\n",
    "cols_to_drop = ['time']\n",
    "X = df.drop(columns=[col_target]+cols_to_drop)\n",
    "y = df[col_target]\n",
    "\n",
    "# split to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8f5ffad-24f9-4d37-8311-791f9d141dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# title + body\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_alltext_cv \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody-text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m test_alltext_cv \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody-text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m (train_title_cv\u001b[38;5;241m.\u001b[39mshape, train_body_cv\u001b[38;5;241m.\u001b[39mshape, \n\u001b[1;32m     17\u001b[0m  test_title_cv\u001b[38;5;241m.\u001b[39mshape, test_body_cv\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     18\u001b[0m  train_alltext_cv\u001b[38;5;241m.\u001b[39mshape, test_alltext_cv\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1387\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1389\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:234\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    231\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# get count vectorize tables\n",
    "cv = CountVectorizer(token_pattern=ipyutils.PAT_TOKEN)\n",
    "\n",
    "# title\n",
    "train_title_cv = cv.fit_transform(X_train['title'])\n",
    "test_title_cv = cv.transform(X_test['title'])\n",
    "\n",
    "# body\n",
    "train_body_cv = cv.fit_transform(X_train['body-text'])\n",
    "test_body_cv = cv.transform(X_test['body-text'])\n",
    "\n",
    "# title + body\n",
    "alltext\n",
    "train_alltext_cv = cv.fit_transform(X_train['title'] + ' ' + X_train['body-text'])\n",
    "test_alltext_cv = cv.transform(X_test['title'] + ' ' + X_train['body-text'])\n",
    "\n",
    "(train_title_cv.shape, train_body_cv.shape, \n",
    " test_title_cv.shape, test_body_cv.shape,\n",
    " train_alltext_cv.shape, test_alltext_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae3f78-386f-4558-8961-a431a950d9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95f81ff7-f431-4ce8-9fb4-f9f1e0b221bc",
   "metadata": {},
   "source": [
    "### 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf461c2-0d8d-410d-b225-dda2bcca19ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
